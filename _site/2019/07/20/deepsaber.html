<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="google-site-verification" content="iJ3bc1hdJ7bEpE5lbDoHopnvFezdT5kx0VFo2-H7ycM" />
    <meta name="google-site-verification" content="vq8goVU_JDfJuz6fciLnx-WBUJE1H5oAwpPILuOukns" />

    <title>OxAI - Oxford Artificial Intelligence Society</title>

    <link href="/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="/css/grayscale.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <!-- <link title="timeline-styles" rel="stylesheet" href="https://cdn.knightlab.com/libs/timeline3/latest/css/timeline.css"> -->
    <link href="/css/style.css" rel="stylesheet">
    <link href="/css/events.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/styles.css">
    <link rel="stylesheet" href="/css/jquery.qtip.min.css">
    <!-- <link rel="stylesheet" href="/css/fullcalendar.min.css"> -->
    <link href='/js/packages/core/main.css' rel='stylesheet' />
    <link href='/js/packages/daygrid/main.css' rel='stylesheet' />
    <link href='/js/packages/list/main.css' rel='stylesheet' />
    

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <meta name="description" content="The mission of the Oxford Artificial Intelligence Society is to provide a platform for the interaction of researchers, students, and professionals interested in Artificial Intelligence.">
    <meta http-equiv="content-type" content="text/html;charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">


</head>

<!--Here begins the website itself-->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <script type="text/javascript">
        var isBlog = false; //For width/height changes to TweenLite
    </script>

    <!-- Navigation (navigation bar on the top)-->

    <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="/#page-top">
      <img id="minilogo" src="/img/logo2.png"></img>
        <span class="light">&nbsp;OX</span>AI
    </a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/events">Events</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/join">Join</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/labs">Labs</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blog">Blog</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/#sponsors">Sponsors</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/#partners">Partners</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/team">Team</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


    <!-- <style>
    .blog{
        width: 84%;
        margin: 0 auto;
    }
    .blog .title{
        font-size: 24px;
        margin-top: 18px;
        margin-bottom: 8px;
    }
    .blog .content p{
        font-size: 1.2em;
        text-align: left;
    }
    .blog .content .author{
        text-align: right;
    }
    #logo{
        margin-top: 35px;
    }
    @media only screen and (max-width : 768px) {
        .blog{
            width: 90%;
            margin: 0 auto;
        }
        .blog .title{
            font-size: 1.2em;
            margin-bottom: 28px;
        }
        .blog .content p{
            font-size: 1em;
        }
    }
</style> -->

<section class="page blog">
  <div class="container mt-5">
      <div class="row text-center">
        <div class="col-lg-8 mx-auto">
          <h2 class="text-white mb-2">DeepSaber: Generating Beat Saber levels using Machine Learning</h2>
          <h4 class="text-white mb-4">20 Jul 2019</h2>
        </div>
      </div>

      <div class="row w-75 mx-auto">

      <p>During Hilary and Trinity Term 2019, OxAI Labs piloted its premier research project with a team of seven mixed-discipline Oxford students. The objective of the team was to investigate the development of an Artificial Intelligence (AI) level generator for the <a href="https://beatsaber.com/">Beat Saber</a> Virtual Reality (VR) game.</p>

<p>Beat Saber (May 2018) is a music-based VR game where players use hand-held motion trackers to slash through incoming blocks, positioned according to the rhythm and melody of a song. The player can also be expected to move to avoid obstacles and explosive blocks as part of a song. 
Beat Saber gameplay might be compared to Guitar Hero and Fruit Ninja (video here), as a mixed rhythm game inclusive of saber dynamics. Beat Saber has become immensely popular in the VR community, however on release it only provided a limited number of song levels to play. This has led to a great contribution from the mod community to introduce custom songs to Beat Saber (i.e. Beatsaver Link and others). These community-led projects facilitate players to create song levels via an editor as well as export them and share them with the world.</p>

<p>The challenge of automatically generating fun and rhythmically coherent Beat Saber levels caught the interest of OxAI Labs as an interesting intersection of machine learning and audio processing, realisable in a virtual-reality setting. What’s more, a successful investigation would stand to proffer the devoted Beat Saber community with an additional tool for a continuing enjoyment of the game.</p>

<p>I am happy to share with you the illustrations of our process, the winding considerations, challenges and successes. This project would not have been possible were it not for the amazing team, Andrea, Guillermo, JJ, Mackenzie, Michael, Tim and Ralph (myself). The creativity and drive of each member inspired a great collaboration, of which the fruits shall be shared forthcoming.</p>

<p>At the start of our project we began, as could well be expected from a team of AI nerds, by defining level generation in a computational way. For human creators, level creation consists of placing blocks and obstacles at “events” in the song, corresponding to beat drops, melody changes, etc., such that block sequences follow song-specific patterns. Hence, we defined our AI level generator’s task as learning to map raw waveforms (how computers process audio) to Beat Saber levels from human-created examples, such that this mapping approximates human judgement for musical “events” as reliably as possible. To this end, we collected a large dataset of levels from BeatSaver.com and collected the 1000 most downloaded levels (as of February 18th, 2019 ). We augmented our data set by shifting in the frequency domain to obtain 8 times as many training samples.</p>

<p>Using this collected data, we began looking at representations and features for raw audio that can capture these musical events, and looked at related work in speech recognition and audio processing. There, raw audio, itself too bulky to be efficiently analysed, is converted to more compressed features such as <u title="Mel spectrograms return amplitudes of a small number of logarithmically-scaled frequency windows across a range of audio samples">Mel spectrograms</u> and <u title="Chromagrams returns the amplitudes of frequencies within windows around all 12 chromatic scale frequencies (C, C#, D, etc.)">chromagrams</u>, and then used for downstream tasks. Both spectrograms and chromagrams relay events such as beat drops and note changes, which are important for level generation. Therefore, we experimented with both features during our work.</p>

<p>We then defined representations for Beat Saber levels that can concisely, yet comprehensively, capture a wide range of block timings and block combinations.  In theory, blocks/obstacles can appear at any real-valued time, which makes generation highly intractable, so we discretised time to intervals small enough not to be noticeable by humans. We came to realise that even with just blocks and bombs, there are <u title="9 possibilities (8 directions plus directionless block) per hand, plus bomb and empty cell">20</u> possibilities for a single cell which, with Beat Saber’s 4x3 grid, yields 2012 (≈4.1 x 1015 ) theoretically possible states at any given time! In our earlier models, we avoided this exponential blow-up by introducing independence between cells. Later, we modelled the full block configuration space such that any configuration can still theoretically be returned. However, we also introduced a new state representation using only the top 2000 configurations of blocks in the training set, which accounted for 99.53% of all configurations. This dramatically simplified the output space, and provided the model with a strong inductive bias towards human levels, albeit at the expense of not producing original new configurations.</p>

<p>We began the process of implementing models over the course of several weekly meetings and weekend work sessions. We started with a simple rule-based system that would create blocks based on dominant chroma, and built upon on that by restricting the size of the state-space and adding some stochastic decisions, with unit-testing checks to ensure level compliance. We built our first machine learning model towards the end of March, an adapted non-causal WaveNet model, and trained using maximum likelihood. Our model used a time discretization that was relative to the tempo (Beats Per Minute) of the input song (1/16th of a beat), and performed in a manner that was encouraging, though not entirely convincingly. We tried using Generative Adversarial Networks (GANs) with this model to incorporate a sense of “authenticity” into the training process, but did not observe any dramatic improvements, especially given the additional computational costs versus benefit of this change.</p>

<p>Towards the start of Trinity, inspired by similar work for the Dance Dance Revolution game (Dance Dance Convolution (DDC)), we developed a new two-stage model for level generation that divides the task into 2 parts: block placement (when to do something) and block selection (what to do). The first component of this model returns a binary “place something” flag at every time discretisation, and the second component chooses a configuration for flagged times given audio context.<br />
We adopted and adapted the DDC model by using our earlier WaveNet model for the first stage, supplementing it with a peak-selection algorithm to avoid consecutive “place something” cues, and a <a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a> for the second stage. We found that, given the modularity of this model, we were also able to use the first stage of the original DDC with our second-stage transformer to perform additional experiments without much additional labour.</p>

<p>Nearing the culmination of the project we trained our end-to-end WaveNet and two-stage models in two cloud instances containing V100 GPUs. These resources were made available with the support of a computational resources grant from the Google Cloud Platform. We experimented with a number of setups, especially for the first stage of the two-stage model. Ultimately, our (subjectively judged) best-performing model was the two-stage model, containing the first stage architecture of Dance Dance Convolution. This model produced realistic levels with mostly reasonable timings and block patterns. It could also produce levels of variable difficulty, simply by varying the sensitivity of the block placing stage of the algorithm.</p>

<p>On May 29th, OxAI Labs hosted its premier demo day to present the results of the Beat Saber project in Trinity College, Oxford. The demonstration consisted of a 40-minute presentation followed by a demo on a VR headset for attendees to try out AI-generated levels. We also presented the Beat Saber project at the Oxford Immersive Technologies event at the Mathematics Institute on the 21st of June, and were grateful to receive overwhelmingly positive feedback. We aim to make our models, data and code open-source before the end of August 2019 and following that, to develop an interface through which Beat Saber players can easily use our tool. Some of the project members have expressed an interest to pursue further research into incorporating difficulty and playability analysis as part of the generation process, by using metadata available online about human-created levels.</p>

<p>The completition of the Beat Saber project represents OxAI Labs’ first major enterprise into community-led research. The project was intended to encourage collaboration and learning and I am very happy to confirm the value that we have each taken away during its pursuit. We aim to build on the success of Beat Saber and will soon be announcing new projects for Michaelmas 2019. We look forward to hosting new community collaborations soon and are very excited for the developments that lie ahead.</p>

<p>Make sure to Subscribe to the OxAI Labs newsletter if you are interested in participating in a future OxAI Labs Research Project.</p>


      <p class="author">Written by
      <br><i>Ralph Abboud</i>
      <br><i>ralph.abboud@oxai.org</i></p>

      </div>
</div>
</section>

    <!--Welcome section-->
    <!-- <div id="large-header" class="large-header container">
      <canvas id="demo-canvas"></canvas>
      <div class="row welcome">
        <div class="welcome2">
          <img id="logo" src="/img/logo.png"/>
            <div class="blog">
                <h3 class="title">
                    DeepSaber: Generating Beat Saber levels using Machine Learning - 20 Jul 2019
                </h3>
                <div class="content">

                    <p>During Hilary and Trinity Term 2019, OxAI Labs piloted its premier research project with a team of seven mixed-discipline Oxford students. The objective of the team was to investigate the development of an Artificial Intelligence (AI) level generator for the <a href="https://beatsaber.com/">Beat Saber</a> Virtual Reality (VR) game.</p>

<p>Beat Saber (May 2018) is a music-based VR game where players use hand-held motion trackers to slash through incoming blocks, positioned according to the rhythm and melody of a song. The player can also be expected to move to avoid obstacles and explosive blocks as part of a song. 
Beat Saber gameplay might be compared to Guitar Hero and Fruit Ninja (video here), as a mixed rhythm game inclusive of saber dynamics. Beat Saber has become immensely popular in the VR community, however on release it only provided a limited number of song levels to play. This has led to a great contribution from the mod community to introduce custom songs to Beat Saber (i.e. Beatsaver Link and others). These community-led projects facilitate players to create song levels via an editor as well as export them and share them with the world.</p>

<p>The challenge of automatically generating fun and rhythmically coherent Beat Saber levels caught the interest of OxAI Labs as an interesting intersection of machine learning and audio processing, realisable in a virtual-reality setting. What’s more, a successful investigation would stand to proffer the devoted Beat Saber community with an additional tool for a continuing enjoyment of the game.</p>

<p>I am happy to share with you the illustrations of our process, the winding considerations, challenges and successes. This project would not have been possible were it not for the amazing team, Andrea, Guillermo, JJ, Mackenzie, Michael, Tim and Ralph (myself). The creativity and drive of each member inspired a great collaboration, of which the fruits shall be shared forthcoming.</p>

<p>At the start of our project we began, as could well be expected from a team of AI nerds, by defining level generation in a computational way. For human creators, level creation consists of placing blocks and obstacles at “events” in the song, corresponding to beat drops, melody changes, etc., such that block sequences follow song-specific patterns. Hence, we defined our AI level generator’s task as learning to map raw waveforms (how computers process audio) to Beat Saber levels from human-created examples, such that this mapping approximates human judgement for musical “events” as reliably as possible. To this end, we collected a large dataset of levels from BeatSaver.com and collected the 1000 most downloaded levels (as of February 18th, 2019 ). We augmented our data set by shifting in the frequency domain to obtain 8 times as many training samples.</p>

<p>Using this collected data, we began looking at representations and features for raw audio that can capture these musical events, and looked at related work in speech recognition and audio processing. There, raw audio, itself too bulky to be efficiently analysed, is converted to more compressed features such as <u title="Mel spectrograms return amplitudes of a small number of logarithmically-scaled frequency windows across a range of audio samples">Mel spectrograms</u> and <u title="Chromagrams returns the amplitudes of frequencies within windows around all 12 chromatic scale frequencies (C, C#, D, etc.)">chromagrams</u>, and then used for downstream tasks. Both spectrograms and chromagrams relay events such as beat drops and note changes, which are important for level generation. Therefore, we experimented with both features during our work.</p>

<p>We then defined representations for Beat Saber levels that can concisely, yet comprehensively, capture a wide range of block timings and block combinations.  In theory, blocks/obstacles can appear at any real-valued time, which makes generation highly intractable, so we discretised time to intervals small enough not to be noticeable by humans. We came to realise that even with just blocks and bombs, there are <u title="9 possibilities (8 directions plus directionless block) per hand, plus bomb and empty cell">20</u> possibilities for a single cell which, with Beat Saber’s 4x3 grid, yields 2012 (≈4.1 x 1015 ) theoretically possible states at any given time! In our earlier models, we avoided this exponential blow-up by introducing independence between cells. Later, we modelled the full block configuration space such that any configuration can still theoretically be returned. However, we also introduced a new state representation using only the top 2000 configurations of blocks in the training set, which accounted for 99.53% of all configurations. This dramatically simplified the output space, and provided the model with a strong inductive bias towards human levels, albeit at the expense of not producing original new configurations.</p>

<p>We began the process of implementing models over the course of several weekly meetings and weekend work sessions. We started with a simple rule-based system that would create blocks based on dominant chroma, and built upon on that by restricting the size of the state-space and adding some stochastic decisions, with unit-testing checks to ensure level compliance. We built our first machine learning model towards the end of March, an adapted non-causal WaveNet model, and trained using maximum likelihood. Our model used a time discretization that was relative to the tempo (Beats Per Minute) of the input song (1/16th of a beat), and performed in a manner that was encouraging, though not entirely convincingly. We tried using Generative Adversarial Networks (GANs) with this model to incorporate a sense of “authenticity” into the training process, but did not observe any dramatic improvements, especially given the additional computational costs versus benefit of this change.</p>

<p>Towards the start of Trinity, inspired by similar work for the Dance Dance Revolution game (Dance Dance Convolution (DDC)), we developed a new two-stage model for level generation that divides the task into 2 parts: block placement (when to do something) and block selection (what to do). The first component of this model returns a binary “place something” flag at every time discretisation, and the second component chooses a configuration for flagged times given audio context.<br />
We adopted and adapted the DDC model by using our earlier WaveNet model for the first stage, supplementing it with a peak-selection algorithm to avoid consecutive “place something” cues, and a <a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a> for the second stage. We found that, given the modularity of this model, we were also able to use the first stage of the original DDC with our second-stage transformer to perform additional experiments without much additional labour.</p>

<p>Nearing the culmination of the project we trained our end-to-end WaveNet and two-stage models in two cloud instances containing V100 GPUs. These resources were made available with the support of a computational resources grant from the Google Cloud Platform. We experimented with a number of setups, especially for the first stage of the two-stage model. Ultimately, our (subjectively judged) best-performing model was the two-stage model, containing the first stage architecture of Dance Dance Convolution. This model produced realistic levels with mostly reasonable timings and block patterns. It could also produce levels of variable difficulty, simply by varying the sensitivity of the block placing stage of the algorithm.</p>

<p>On May 29th, OxAI Labs hosted its premier demo day to present the results of the Beat Saber project in Trinity College, Oxford. The demonstration consisted of a 40-minute presentation followed by a demo on a VR headset for attendees to try out AI-generated levels. We also presented the Beat Saber project at the Oxford Immersive Technologies event at the Mathematics Institute on the 21st of June, and were grateful to receive overwhelmingly positive feedback. We aim to make our models, data and code open-source before the end of August 2019 and following that, to develop an interface through which Beat Saber players can easily use our tool. Some of the project members have expressed an interest to pursue further research into incorporating difficulty and playability analysis as part of the generation process, by using metadata available online about human-created levels.</p>

<p>The completition of the Beat Saber project represents OxAI Labs’ first major enterprise into community-led research. The project was intended to encourage collaboration and learning and I am very happy to confirm the value that we have each taken away during its pursuit. We aim to build on the success of Beat Saber and will soon be announcing new projects for Michaelmas 2019. We look forward to hosting new community collaborations soon and are very excited for the developments that lie ahead.</p>

<p>Make sure to Subscribe to the OxAI Labs newsletter if you are interested in participating in a future OxAI Labs Research Project.</p>


                    <p class="author">Written by
                    <br><i>Ralph Abboud</i>
                    <br><i>ralph.abboud@oxai.org</i></p>
                </div>
            </div>
        </div>
    </div>
</div> -->


<!-- <script type="text/javascript">
    var isBlog = true; //For width/height changes to TweenLite
</script> -->


    <script src="/js/lazysizes.min.js" async=""></script>
    <!-- Custom Theme JavaScript -->
    <script src="/vendor/jquery/jquery.min.js"></script>
    <!-- <script src="/js/grayscale.js"></script> -->
    <script type="text/javascript" src="/js/TweenLite.min.js"></script>

    <!-- Footer -->
    <footer class="bg-black small text-center text-white-50">
      <div class="container">
        Copyright &copy; OxAI 2019
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="/vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="/js/grayscale.min.js"></script>

    <!-- Full calendar -->
    <script src='js/moment.min.js'></script>
    <script src='/js/jquery.qtip.min.js'></script>
    <!-- <script src='/js/fullcalendar/fullcalendar.min.js'></script>
    <script src='/js/fullcalendar/gcal.js'></script> -->

    <script src='/js/packages/core/main.js'></script>
    <script src='/js/packages/interaction/main.js'></script>
    <script src='/js/packages/daygrid/main.js'></script>
    <script src='/js/packages/list/main.js'></script>
    <script src='/js/packages/google-calendar/main.js'></script>

    <script src="/js/script.js"></script>

</body>

</html>
